{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBt8ma8Rbj_O"
      },
      "outputs": [],
      "source": [
        "## source: https://towardsdatascience.com/word-embeddings-for-sentiment-analysis-65f42ea5d26e\n",
        "## source: https://towardsdatascience.com/twitter-topic-modeling-e0e3315b12e2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras.preprocessing.text\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKau32BsuBO3",
        "outputId": "a0c16845-c3f1-4b0e-9d8b-91d540bd912f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras.preprocessing.text (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for keras.preprocessing.text\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.0\n",
            "    Uninstalling scipy-1.16.0:\n",
            "      Successfully uninstalled scipy-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "d03fa6148fd040d0bf46862061270430"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import gzip\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# הגדרות\n",
        "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz'\n",
        "gz_path = 'cc.id.300.vec.gz'\n",
        "vec_path = 'cc.id.300.vec'\n",
        "\n",
        "# הורדה אם לא קיים\n",
        "if not os.path.exists(gz_path):\n",
        "    print(\"⬇️ Downloading Indo FastText embedding...\")\n",
        "    urllib.request.urlretrieve(url, gz_path)\n",
        "    print(\"✅ Download complete!\")\n",
        "\n",
        "# חילוץ הקובץ מ-gzip אם צריך\n",
        "if not os.path.exists(vec_path):\n",
        "    print(\"🧩 Extracting .vec file from .gz...\")\n",
        "    with gzip.open(gz_path, 'rb') as f_in:\n",
        "        with open(vec_path, 'wb') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "    print(\"✅ Extraction complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OY322bFpR778",
        "outputId": "4fc16b89-95ef-48c1-abed-270746b549c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3089261566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# הגדרות\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQn4V5H-ibIo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import keras\n",
        "from keras import layers, regularizers, Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import Accuracy\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxl2v7heFySU"
      },
      "outputs": [],
      "source": [
        "def custom_tokenizer(df, tk):\n",
        "    \"\"\"\n",
        "    Tokenizes tweets using Keras' Tokenizer after stop word removal\n",
        "\n",
        "    Args:     df, Pandas DataFrame: Contains all tweets and labels\n",
        "              tk, Keras Tokenizer Object:  Creates tokens via fit_on_texts fn\n",
        "\n",
        "    Returns:  df, Pandas DataFrame:  Updated DataFrame with tokenized tweets in new column\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizeList = []\n",
        "\n",
        "    # remove stopwords\n",
        "    df.tweet = df.tweet.apply(remove_stopwords)\n",
        "\n",
        "    tweetList = df['tweet'].tolist()\n",
        "    tk.fit_on_texts(df['tweet'])\n",
        "    inv_map = {v: k for k, v in tk.word_index.items()}\n",
        "\n",
        "    for sentence in tweetList:\n",
        "        tweet = re.split('\\s+', sentence)\n",
        "        processed_seq = tk.texts_to_sequences(tweet)\n",
        "        tokens = [inv_map[tok] for seq in processed_seq for tok in seq]\n",
        "        tokenizeList.append(tokens)\n",
        "\n",
        "    df['tokens'] = tokenizeList\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPYHwmvj2DXW"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(input_text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from tweets based on Indonesian stop word list\n",
        "\n",
        "    Args:     input_text, string:  tweet\n",
        "\n",
        "    Returns:  string: cleaned tweet\n",
        "    \"\"\"\n",
        "\n",
        "    stopwords_list = stopwords.words('indonesian')\n",
        "\n",
        "    words = input_text.split()\n",
        "    clean_words = [word for word in words if word not in stopwords_list]\n",
        "\n",
        "    return \" \".join(clean_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1QMmLpTibIq"
      },
      "outputs": [],
      "source": [
        "def embedding_to_matrix(token_dict, embeddings, dimensionality, NB_WORDS):\n",
        "    \"\"\"\n",
        "    Converts Pandas df of pre-trained embeddings to a NxN matrix cross-referenced with\n",
        "    all tokens in corpus.\n",
        "\n",
        "    Args:     token_dict, dict: dictionary of tokens representing all tweets\n",
        "              Embeddings, Pandas DataFrame: pre-trained Embeddings\n",
        "              Dimensionality, int: dimension of embeddings (i.e. 400-D for Word2Vec or 100-D for FT)\n",
        "              NB_WORDS, int: Parameter indicating the number of words we'll put in the embedding dictionary\n",
        "\n",
        "    Returns:  emb_matrix, NumPy Array:  NxN matrix cross-referenced with all tokens in corpus\n",
        "    \"\"\"\n",
        "\n",
        "    emb_dict = {}\n",
        "\n",
        "    for line in embeddings:\n",
        "        word = line[0]\n",
        "        vector = np.asarray(line[1:], dtype='float32')\n",
        "        emb_dict[word] = vector\n",
        "\n",
        "    emb_matrix = np.zeros((NB_WORDS, dimensionality))\n",
        "\n",
        "    for w, i in token_dict:\n",
        "        # The word_index contains a token for all words of the training data so we need to limit that\n",
        "        if i < NB_WORDS:\n",
        "            vect = emb_dict.get(w)\n",
        "            # Check if the word from the training data occurs in the pre-trained word embeddings\n",
        "            # Otherwise the vector is kept with only zeros\n",
        "            if vect is not None:\n",
        "                emb_matrix[i] = vect\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return emb_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x78rDylaibIr"
      },
      "outputs": [],
      "source": [
        "def dict_of_tokens(df, tk):\n",
        "    \"\"\"\n",
        "    Creates a dictionary of tokens found in the tweets.\n",
        "\n",
        "    Args:     df, Pandas Dataframe: Contains all tweets and labels\n",
        "              tk, Keras Tokenizer Object:  Creates dict of tokens via fit_on_texts fn\n",
        "\n",
        "    Returns:  dictionary of tokens\n",
        "    \"\"\"\n",
        "\n",
        "    tk.fit_on_texts(df['tweet'])\n",
        "\n",
        "    return tk.word_index.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTFihg3bibIs"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_sequences(df, tk):\n",
        "    \"\"\"\n",
        "    Converts tokenized tweets to sequences.\n",
        "\n",
        "    Args:       df, Pandas Dataframe: Contains all tweets, tokenized tweets and labels\n",
        "                tk, Keras Tokenizer Object:  Converts tokenized tweets to sequences via texts_to_sequences fn\n",
        "\n",
        "    Returns:    NumPy array:  List of all tokenized tweets represented as a sequence of numbers\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = df['tokens']\n",
        "\n",
        "    return tk.texts_to_sequences(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTrPwRy9ibIt"
      },
      "outputs": [],
      "source": [
        "def import_embedding(filepath, Dimensionality):\n",
        "    \"\"\"\n",
        "    Imports a pre-trained embedding file and converts to NumPy array.\n",
        "\n",
        "    Args:       filepath, string:  file path for embedding data\n",
        "                Dimensionality, int: dimension of embeddings (i.e. 400-D for Word2Vec or 100-D for FT)\n",
        "\n",
        "    Returns:    NumPy array:  pre-trained embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    header_list = list(range(0, Dimensionality+1))\n",
        "\n",
        "    embeddings = pd.read_csv(filepath, delimiter=' ', skiprows=1, index_col=False, names=header_list)\n",
        "\n",
        "    return embeddings.to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiG8Fl1GibIu"
      },
      "outputs": [],
      "source": [
        "def sequence_pad(X_seq):\n",
        "    \"\"\"\n",
        "    Pads the sequences to some length of max tweet in tokens.\n",
        "\n",
        "    Args:       X_seq, NumPy array:  List of all tokenized tweets represented as a sequence of numbers\n",
        "\n",
        "    Returns:    X_seq_trunc, NumPy array:  List of all tokenized tweets appended to value of max tweet length\n",
        "                MAX_LEN, int:   Maximum tweet length in tokens\n",
        "    \"\"\"\n",
        "\n",
        "    lengths = []\n",
        "\n",
        "    for i in X_seq:\n",
        "        lengths.append(len(i))\n",
        "\n",
        "    MAX_LEN = max(lengths)\n",
        "\n",
        "    X_seq_trunc = pad_sequences(X_seq, maxlen=MAX_LEN)\n",
        "\n",
        "    return X_seq_trunc, MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r6ZXoHfibIu"
      },
      "outputs": [],
      "source": [
        "def encode_labels(df):\n",
        "    \"\"\"\n",
        "    One-hot encodes the emotion labels.\n",
        "\n",
        "    Args:       df, Pandas DataFrame:  Contains all tweets and labels\n",
        "\n",
        "    Returns:    y_oh, NumPy array:  one-hot encoded emotion labels\n",
        "    \"\"\"\n",
        "\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    y = le.fit_transform(df['label'])\n",
        "\n",
        "    return to_categorical(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27wOywUpibIv"
      },
      "outputs": [],
      "source": [
        "def deep_model(model, X_train, y_train, X_valid, y_valid, checkpoint_filepath):\n",
        "    \"\"\"\n",
        "    Function to used for individual model exploration.\n",
        "    Saves file path at epoch with max validation accuracy.\n",
        "\n",
        "    Args:       model, Keras object: model with the chosen architecture\n",
        "                X_train, NumPy array: training features\n",
        "                y_train, NumPy array: training target\n",
        "                X_valid, NumPy array: validation features\n",
        "                Y_valid, NumPy array: validation target\n",
        "                checkpoint_filepath:  file path to save epoch with max validation accuracy\n",
        "\n",
        "    Returns:    model training history, Keras object: contains the output of the Keras model.fit fn\n",
        "    \"\"\"\n",
        "\n",
        "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "                            filepath=checkpoint_filepath,\n",
        "                            save_weights_only=False,\n",
        "                            monitor='val_accuracy',\n",
        "                            mode='max',\n",
        "                            save_best_only=True)\n",
        "\n",
        "    model.compile(optimizer='rmsprop'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train\n",
        "                       , y_train\n",
        "                       , epochs=NB_START_EPOCHS\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=2\n",
        "                       , callbacks=[model_checkpoint_callback]\n",
        "                       , shuffle=True )\n",
        "    return history\n",
        "\n",
        "\n",
        "def eval_metric(history, metric_name):\n",
        "    '''\n",
        "    Function to evaluate a trained model on a chosen metric.\n",
        "    Training and validation metric are plotted in a\n",
        "    line chart for each epoch.\n",
        "\n",
        "    Args:     history, Keras object: model training history\n",
        "              metric_name, string: loss or accuracy\n",
        "\n",
        "    Returns:  matplotlib chart:  line chart with epochs of x-axis and metric on y-axis\n",
        "    '''\n",
        "    metric = history.history[metric_name]\n",
        "    val_metric = history.history['val_' + metric_name]\n",
        "\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "\n",
        "    plt.plot(e, metric, label='Train ' + metric_name)\n",
        "    plt.plot(e, val_metric, label='Validation ' + metric_name)\n",
        "    plt.title('Bi-LSTM ' + metric_name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.xticks(np.arange(min(e)+1, max(e)+1, 2.0))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EbRlVt4yVqY"
      },
      "outputs": [],
      "source": [
        "def make_classification_report(X_valid, y_valid, checkpoint_filepath):\n",
        "    \"\"\"\n",
        "    Makes a classification report.\n",
        "\n",
        "    Args:     X_valid, NumPy array: validation features\n",
        "              Y_valid, NumPy array: validation target\n",
        "              checkpoint_filepath:  file path to save epoch with max validation accuracy\n",
        "\n",
        "    Returns:  classification report\n",
        "    \"\"\"\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "\n",
        "    label_names = [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\"]\n",
        "\n",
        "    y_prob = model.predict(X_valid)\n",
        "    prediction_ints = np.zeros_like(y_prob)\n",
        "    prediction_ints[np.arange(len(y_prob)), y_prob.argmax(1)] = 1\n",
        "    prediction = np.where(prediction_ints==1)[1]\n",
        "\n",
        "    return print(classification_report(y_valid, prediction_ints, target_names=label_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO_fsE7H0ETF"
      },
      "outputs": [],
      "source": [
        "def make_confusion_matrix(X_valid, y_valid, checkpoint_filepath):\n",
        "    \"\"\"\n",
        "    Makes a confusion matrix.\n",
        "\n",
        "    Args:     X_valid, NumPy array: validation features\n",
        "              Y_valid, NumPy array: validation target\n",
        "              checkpoint_filepath:  file path to save epoch with max validation accuracy\n",
        "\n",
        "    Returns:  confusion matrix\n",
        "    \"\"\"\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "\n",
        "    label_names = [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\"]\n",
        "\n",
        "    y_prob = model.predict(X_valid)\n",
        "    prediction_ints = np.zeros_like(y_prob)\n",
        "    prediction_ints[np.arange(len(y_prob)), y_prob.argmax(1)] = 1\n",
        "    prediction = np.where(prediction_ints==1)[1]\n",
        "\n",
        "    y_cat_valid_emb = np.where(y_valid==1)[1]\n",
        "\n",
        "    cf_matrix = confusion_matrix(prediction, y_cat_valid_emb)\n",
        "\n",
        "    cf_matrix_norm = cf_matrix / cf_matrix.astype(float).sum(axis=1, keepdims=True)\n",
        "\n",
        "    cf_matrix_norm_round = np.around(cf_matrix_norm, decimals=2)\n",
        "\n",
        "    df_cm = pd.DataFrame(cf_matrix_norm_round, columns=label_names, index=label_names)\n",
        "\n",
        "    df_cm.index.name = 'Actual'\n",
        "    df_cm.columns.name = 'Predicted'\n",
        "\n",
        "    sns.heatmap(df_cm, cmap='Blues', annot=True)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc1lMkp1ibIw"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUK77OJeibIw"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOBm5ayvibIw"
      },
      "outputs": [],
      "source": [
        "NB_WORDS = 20000  # Parameter indicating the number of words to be put in the dictionary\n",
        "NB_START_EPOCHS = 20  # Number of epochs used for training\n",
        "BATCH_SIZE = 64  # Size of the batches used in the mini-batch gradient descent\n",
        "MAX_LEN = 64  # Maximum number of words in a sequence\n",
        "WORD2VEC_DIM = 400  # Number of dimensions of the pre-trained word2vec embeddings\n",
        "FASTEXT_DIM = 400 # Number of dimensions of the pre-trained fasttext embeddings\n",
        "NUM_FOLDS = 10 # Number of folds in K-Fold Cross Validation experiment\n",
        "\n",
        "checkpoint_filepath = '/content/checkpoints.keras'\n",
        "tweets_filepath = '/content/Twitter_Emotion_Dataset.csv'\n",
        "word2vec_filepath = '/content/Word2Vec_400dim.txt'\n",
        "fastext_filepath = word2vec_filepath\n",
        "TRAIN_SIZE = .9\n",
        "VALID_SIZE = .1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO8ZpJURibIx"
      },
      "source": [
        "## Data Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1puI5jgAibIx"
      },
      "source": [
        "### File Read-In"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUw0OnyMibIx"
      },
      "outputs": [],
      "source": [
        "# import tweets\n",
        "df = pd.read_csv(tweets_filepath)\n",
        "\n",
        "# import word2vec pre-trained embeddings\n",
        "w2v_embeddings = import_embedding(word2vec_filepath, WORD2VEC_DIM)\n",
        "\n",
        "# # import fasttext pre-trained embeddings\n",
        "# ft_embeddings = import_embedding(fastext_filepath, FASTEXT_DIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwrmZbjFibIy"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16yh9zsOFlcP"
      },
      "outputs": [],
      "source": [
        "# initialize the tokenizer\n",
        "\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "\n",
        "# tokenize tweets and remove stopwords\n",
        "df = custom_tokenizer(df, tk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK_tykU8ibIz"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucWr1GiCibIz"
      },
      "outputs": [],
      "source": [
        "y_oh = encode_labels(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo4k23aRibI0"
      },
      "source": [
        "### Create Token Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyxR4mZ5ibI0"
      },
      "outputs": [],
      "source": [
        "token_dict = dict_of_tokens(df, tk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUKvwjP4ibI0"
      },
      "source": [
        "### Convert data to numerical sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqHz6i-ibI0"
      },
      "outputs": [],
      "source": [
        "X_seq = convert_text_to_sequences(df, tk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaPX9x0ibI0"
      },
      "source": [
        "### Create word sequences of equal length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRCOs8GSibI0"
      },
      "outputs": [],
      "source": [
        "X_seq_trunc, MAX_LEN = sequence_pad(X_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alFtUKa6ibI1"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBA6uDG_ibI1"
      },
      "source": [
        "### Embeddings (Word2Vec and FasText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ei8vvdHibI1",
        "outputId": "17af284c-9b37-4635-809f-a8c1e58980b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'embedding_to_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1987275702.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_emb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWORD2VEC_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mft_emb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFASTEXT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_to_matrix' is not defined"
          ]
        }
      ],
      "source": [
        "w2v_emb_matrix = embedding_to_matrix(token_dict, w2v_embeddings, WORD2VEC_DIM, NB_WORDS)\n",
        "\n",
        "ft_emb_matrix = embedding_to_matrix(token_dict, ft_embeddings, FASTEXT_DIM, NB_WORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fT1ss0RibI1"
      },
      "source": [
        "### Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGws7r24vnC4"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_seq_trunc, y_oh, test_size=VALID_SIZE, random_state=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzC1sB1dibI2"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf_sSMUcwyuf"
      },
      "source": [
        "## Model Examination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPGB3oF6w3uH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(NB_WORDS, FASTEXT_DIM, input_length=MAX_LEN, ))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(layers.Dropout(rate=0.5))\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(layers.Dropout(rate=0.5))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "model.layers[0].trainable = True\n",
        "model_history = deep_model(model, X_train, y_train, X_test, y_test, checkpoint_filepath)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxtO6wGu1q3P"
      },
      "source": [
        "### Model Accuracy Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jscAJ6Jw4fR0"
      },
      "outputs": [],
      "source": [
        "eval_metric(model_history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0mBkIHW1ve1"
      },
      "source": [
        "### Model Loss Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l91pkp854fZq"
      },
      "outputs": [],
      "source": [
        "eval_metric(model_history, 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swOt1Pg-1z8D"
      },
      "source": [
        "### Model Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4Ojovpr4fej"
      },
      "outputs": [],
      "source": [
        "make_classification_report(X_test, y_test, checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0VfLBbL13Fr"
      },
      "source": [
        "### Model Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnXL5Fbeyro5"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(X_test, y_test, checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_LLgkeErol3"
      },
      "source": [
        "## K-Fold Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7Ilh3xG9ZEU"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "kf = KFold(n_splits=NUM_FOLDS)\n",
        "\n",
        "i=0\n",
        "\n",
        "for train_index, test_index in kf.split(X_seq_trunc):\n",
        "    i+=1\n",
        "    print(\"FOLD\", i)\n",
        "    X_train, X_test = X_seq_trunc[train_index], X_seq_trunc[test_index]\n",
        "    y_train, y_test = y_oh[train_index], y_oh[test_index]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(NB_WORDS, FASTEXT_DIM, input_length=MAX_LEN))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "    #model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "    #model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "    #model.add(GRU(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "    model.add(GlobalMaxPool1D())\n",
        "    model.add(Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(rate=0.5))\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(rate=0.5))\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "    # model.layers[0].set_weights([ft_emb_matrix])\n",
        "    model.layers[0].trainable = True\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "\n",
        "    model.fit(X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=NB_START_EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXBlIq0jr30o"
      },
      "source": [
        "## Bootstrap Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x47Zdajr3-5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "bootdf = pd.DataFrame(X_seq_trunc)\n",
        "\n",
        "bootdf['label'] = df['label']\n",
        "\n",
        "for i in range(NUM_FOLDS):\n",
        "\n",
        "    print(\"FOLD\", i)\n",
        "    train = resample(bootdf, replace=True, n_samples=3960, random_state=i)\n",
        "    test = bootdf[~bootdf.index.isin(train.index)]\n",
        "\n",
        "    X_train = train.iloc[:,:-1]\n",
        "    X_test = test.iloc[:,:-1]\n",
        "\n",
        "    y_train = train.iloc[:, -1]\n",
        "    y_test = test.iloc[:, -1]\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_train)\n",
        "    y_train = to_categorical(y)\n",
        "\n",
        "    y2 = le.fit_transform(y_test)\n",
        "    y_test = to_categorical(y2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(NB_WORDS, FASTEXT_DIM, input_length=MAX_LEN))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "    #model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "    #model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "    #model.add(GRU(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "    model.add(GlobalMaxPool1D())\n",
        "    model.add(Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(rate=0.5))\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(rate=0.5))\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "    model.layers[0].set_weights([ft_emb_matrix])\n",
        "    model.layers[0].trainable = True\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "\n",
        "    model.fit(X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=NB_START_EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-5pg5Jr6uml"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SxtO6wGu1q3P",
        "I0mBkIHW1ve1",
        "swOt1Pg-1z8D",
        "s0VfLBbL13Fr"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}