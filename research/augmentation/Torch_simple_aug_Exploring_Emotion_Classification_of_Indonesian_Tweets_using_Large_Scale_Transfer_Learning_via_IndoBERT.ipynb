{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ezERtzrw0fe4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e4665629-7123-46e5-a18c-e233118c3400"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "import  numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "KrGqs7Yc0kCi"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "MODEL_NAME = \"indolem/indobertweet-base-uncased\"\n",
    "LABEL2INDEX = {'love': 0, 'anger': 1, 'sadness': 2, 'happy': 3, 'fear': 4}\n",
    "INDEX2LABEL = {v: k for k, v in LABEL2INDEX.items()}\n",
    "NUM_LABELS = len(LABEL2INDEX)\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AUG_FRAC=0.2\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "04W7XNTN0x0w"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset Class\n",
    "# ------------------------------\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "dN0-eod20zUY"
   },
   "outputs": [],
   "source": [
    "\n",
    "class IndoBertClassifier(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME,\n",
    "                 dense_1=64, dense_2=16, dropout=0.05, num_labels=5):\n",
    "        super(IndoBertClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Layers according to your Keras architecture\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)   # GlobalMaxPool1D equivalent\n",
    "        self.fc1 = nn.Linear(hidden_size, dense_1)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(dense_1, dense_2)\n",
    "        self.fc3 = nn.Linear(dense_2, num_labels)\n",
    "        self.act_relu = nn.ReLU()\n",
    "        self.act_sigmoid = nn.Sigmoid()       # multi-label case\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state  # shape: (batch, seq_len, hidden)\n",
    "\n",
    "        # PyTorch pooling works on (N, C, L), so permute first\n",
    "        x = embeddings.permute(0, 2, 1)        # (batch, hidden, seq_len)\n",
    "        x = self.pool(x).squeeze(-1)           # (batch, hidden)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_relu(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_relu(x)\n",
    "\n",
    "        logits = self.fc3(x)\n",
    "        out = self.act_sigmoid(logits)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "wiuyutkv01EO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Training & Evaluation Functions\n",
    "# ------------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    return total_loss / len(dataloader), acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "WlDgDFmF02fX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    return total_loss / len(dataloader), acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def random_deletion(words, p=0.2):\n",
    "    \"\"\" Randomly delete words with probability p \"\"\"\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    return [w for w in words if random.uniform(0, 1) > p]\n",
    "\n",
    "\n",
    "def random_swap(words, n=1):\n",
    "    \"\"\" Swap two words n times \"\"\"\n",
    "    words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return words\n",
    "\n",
    "\n",
    "def random_insertion(words, n=1):\n",
    "    \"\"\" Insert a random word from the sentence into a random position \"\"\"\n",
    "    words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_word = random.choice(words)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, new_word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def augment_text(text):\n",
    "    words = text.split()\n",
    "    choice = random.choice(['delete', 'swap', 'insert'])\n",
    "\n",
    "    if choice == 'delete':\n",
    "        aug_words = random_deletion(words)\n",
    "    elif choice == 'swap':\n",
    "        aug_words = random_swap(words)\n",
    "    elif choice == 'insert':\n",
    "        aug_words = random_insertion(words)\n",
    "\n",
    "    return \" \".join(aug_words)\n"
   ],
   "metadata": {
    "id": "Afg8bOaQP00p"
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def augment_dataset(df, text_col=\"tweet\", label_col=\"label\", frac=0.1):\n",
    "    \"\"\"\n",
    "    Augment a fraction of dataset rows and return a bigger dataset\n",
    "    \"\"\"\n",
    "    # sample rows for augmentation\n",
    "    sampled = df.sample(frac=frac, random_state=42)\n",
    "\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for _, row in tqdm(sampled.iterrows()):\n",
    "        aug_text = augment_text(row[text_col])  # from previous code\n",
    "        augmented_texts.append(aug_text)\n",
    "        augmented_labels.append(row[label_col])\n",
    "\n",
    "    # make augmented dataframe\n",
    "    df_aug = pd.DataFrame({text_col: augmented_texts, label_col: augmented_labels})\n",
    "\n",
    "    # concatenate with original\n",
    "    df_new = pd.concat([df, df_aug], ignore_index=True)\n",
    "    return df_new\n"
   ],
   "metadata": {
    "id": "vbGjviQ6PovT"
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = \" \".join([y for y in text.split() if y not in indo_stop_words])\n",
    "    return text\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1befXqRQLxX",
    "outputId": "4bd9aca0-7688-42a7-ce6d-e8f4a6c3cc4f"
   },
   "execution_count": 74,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipython-input-375946857.py:5: SyntaxWarning: invalid escape sequence '\\['\n",
      "  text = re.sub('\\[.*?\\]', '', text)\n",
      "/tmp/ipython-input-375946857.py:6: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
      "/tmp/ipython-input-375946857.py:7: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
      "/tmp/ipython-input-375946857.py:11: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  text = re.sub('\\w*\\d\\w*', '', text)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Read dataset (must have \"text\" and \"label\" columns)\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    # df['tweet'] = df['tweet'].apply(lambda x: clean_text(x))\n",
    "    df[\"label\"] = df[\"label\"].map(LABEL2INDEX)\n",
    "\n",
    "    # Split into train/val before augmentation\n",
    "    train_df = df.sample(frac=0.8, random_state=42)\n",
    "    val_df = df.drop(train_df.index)\n",
    "\n",
    ""
   ],
   "metadata": {
    "id": "lcglfCERRo8h"
   },
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "    # Apply augmentation only on train\n",
    "    train_df = augment_dataset(train_df, frac=AUG_FRAC)\n",
    "\n",
    "    # Combine text and labels\n",
    "    train_texts = train_df[\"tweet\"].tolist()\n",
    "    train_labels = train_df[\"label\"].tolist()\n",
    "    val_texts = val_df[\"tweet\"].tolist()\n",
    "    val_labels = val_df[\"label\"].tolist()\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TweetDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TweetDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
   ],
   "metadata": {
    "id": "Fjgv4moKRrbg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5fe3dc65-ad09-4ca9-9f13-84e5c3d4d34f"
   },
   "execution_count": 76,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "697it [00:00, 20280.61it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "    # Model, Loss, Optimizer\n",
    "    model = IndoBertClassifier().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ],
   "metadata": {
    "id": "2pvOx-rqRw25"
   },
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "    # Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save Model\n",
    "    os.makedirs(\"saved_model\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"saved_model/indobert_tweet_classifier_aug.pt\")\n",
    "    print(\"Model saved to saved_model/indobert_tweet_classifier_aug.pt\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0VPBP61Ryqi",
    "outputId": "621780c6-2b6f-49a4-e052-9323120e6b8c"
   },
   "execution_count": 78,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "  Train Loss: 1.5920 | Acc: 0.2704 | F1: 0.1384\n",
      "  Val   Loss: 1.5733 | Acc: 0.3287 | F1: 0.2150\n",
      "Epoch 2/25\n",
      "  Train Loss: 1.4932 | Acc: 0.4179 | F1: 0.3602\n",
      "  Val   Loss: 1.4554 | Acc: 0.4494 | F1: 0.3752\n",
      "Epoch 3/25\n",
      "  Train Loss: 1.3803 | Acc: 0.5336 | F1: 0.4987\n",
      "  Val   Loss: 1.3726 | Acc: 0.5936 | F1: 0.5634\n",
      "Epoch 4/25\n",
      "  Train Loss: 1.2784 | Acc: 0.6333 | F1: 0.5975\n",
      "  Val   Loss: 1.3186 | Acc: 0.6031 | F1: 0.5624\n",
      "Epoch 5/25\n",
      "  Train Loss: 1.1919 | Acc: 0.7119 | F1: 0.6840\n",
      "  Val   Loss: 1.2908 | Acc: 0.6185 | F1: 0.6025\n",
      "Epoch 6/25\n",
      "  Train Loss: 1.1344 | Acc: 0.7877 | F1: 0.7821\n",
      "  Val   Loss: 1.2591 | Acc: 0.7048 | F1: 0.6978\n",
      "Epoch 7/25\n",
      "  Train Loss: 1.0934 | Acc: 0.8743 | F1: 0.8751\n",
      "  Val   Loss: 1.2323 | Acc: 0.7687 | F1: 0.7719\n",
      "Epoch 8/25\n",
      "  Train Loss: 1.0566 | Acc: 0.9273 | F1: 0.9261\n",
      "  Val   Loss: 1.2095 | Acc: 0.7840 | F1: 0.7861\n",
      "Epoch 9/25\n",
      "  Train Loss: 1.0183 | Acc: 0.9421 | F1: 0.9413\n",
      "  Val   Loss: 1.1861 | Acc: 0.7876 | F1: 0.7907\n",
      "Epoch 10/25\n",
      "  Train Loss: 0.9951 | Acc: 0.9496 | F1: 0.9490\n",
      "  Val   Loss: 1.1771 | Acc: 0.7828 | F1: 0.7868\n",
      "Epoch 11/25\n",
      "  Train Loss: 0.9792 | Acc: 0.9591 | F1: 0.9589\n",
      "  Val   Loss: 1.1579 | Acc: 0.7959 | F1: 0.7983\n",
      "Epoch 12/25\n",
      "  Train Loss: 0.9674 | Acc: 0.9617 | F1: 0.9616\n",
      "  Val   Loss: 1.1609 | Acc: 0.7793 | F1: 0.7821\n",
      "Epoch 13/25\n",
      "  Train Loss: 0.9559 | Acc: 0.9711 | F1: 0.9710\n",
      "  Val   Loss: 1.1417 | Acc: 0.8030 | F1: 0.8040\n",
      "Epoch 14/25\n",
      "  Train Loss: 0.9477 | Acc: 0.9737 | F1: 0.9737\n",
      "  Val   Loss: 1.1426 | Acc: 0.7959 | F1: 0.7972\n",
      "Epoch 15/25\n",
      "  Train Loss: 0.9432 | Acc: 0.9751 | F1: 0.9751\n",
      "  Val   Loss: 1.1375 | Acc: 0.8053 | F1: 0.8046\n",
      "Epoch 16/25\n",
      "  Train Loss: 0.9415 | Acc: 0.9742 | F1: 0.9741\n",
      "  Val   Loss: 1.1515 | Acc: 0.7840 | F1: 0.7890\n",
      "Epoch 17/25\n",
      "  Train Loss: 0.9422 | Acc: 0.9723 | F1: 0.9723\n",
      "  Val   Loss: 1.1408 | Acc: 0.7970 | F1: 0.7949\n",
      "Epoch 18/25\n",
      "  Train Loss: 0.9352 | Acc: 0.9768 | F1: 0.9768\n",
      "  Val   Loss: 1.1307 | Acc: 0.8030 | F1: 0.8036\n",
      "Epoch 19/25\n",
      "  Train Loss: 0.9318 | Acc: 0.9792 | F1: 0.9792\n",
      "  Val   Loss: 1.1368 | Acc: 0.7923 | F1: 0.7913\n",
      "Epoch 20/25\n",
      "  Train Loss: 0.9301 | Acc: 0.9809 | F1: 0.9809\n",
      "  Val   Loss: 1.1453 | Acc: 0.7805 | F1: 0.7850\n",
      "Epoch 21/25\n",
      "  Train Loss: 0.9303 | Acc: 0.9792 | F1: 0.9792\n",
      "  Val   Loss: 1.1290 | Acc: 0.8041 | F1: 0.8042\n",
      "Epoch 22/25\n",
      "  Train Loss: 0.9286 | Acc: 0.9806 | F1: 0.9806\n",
      "  Val   Loss: 1.1387 | Acc: 0.7899 | F1: 0.7918\n",
      "Epoch 23/25\n",
      "  Train Loss: 0.9300 | Acc: 0.9799 | F1: 0.9799\n",
      "  Val   Loss: 1.1428 | Acc: 0.7852 | F1: 0.7865\n",
      "Epoch 24/25\n",
      "  Train Loss: 0.9322 | Acc: 0.9761 | F1: 0.9761\n",
      "  Val   Loss: 1.1364 | Acc: 0.7899 | F1: 0.7922\n",
      "Epoch 25/25\n",
      "  Train Loss: 0.9288 | Acc: 0.9794 | F1: 0.9794\n",
      "  Val   Loss: 1.1380 | Acc: 0.7888 | F1: 0.7922\n",
      "Model saved to saved_model/indobert_tweet_classifier.pt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "kPt1IW6y0vKv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
