{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 16295,
     "databundleVersionId": 1099992,
     "sourceType": "competition"
    },
    {
     "sourceId": 1034812,
     "sourceType": "datasetVersion",
     "datasetId": 570327
    },
    {
     "sourceId": 1046215,
     "sourceType": "datasetVersion",
     "datasetId": 570053
    }
   ],
   "dockerImageVersionId": 29860,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Importing Necesseties",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:03:07.221461Z",
     "start_time": "2025-08-16T20:02:53.477487Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r ../resources/requirements.txt\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 1)) (2.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 3)) (3.10.5)\n",
      "Requirement already satisfied: seaborn in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 5)) (6.2.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 6)) (11.3.0)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 7)) (1.9.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 8)) (3.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 9)) (3.8.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 10)) (4.67.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 11)) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 12)) (1.7.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 13)) (3.0.4)\n",
      "Requirement already satisfied: torch in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 14)) (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from -r ../resources/requirements.txt (line 15)) (4.55.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from pandas->-r ../resources/requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from pandas->-r ../resources/requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from pandas->-r ../resources/requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from matplotlib->-r ../resources/requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from matplotlib->-r ../resources/requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from matplotlib->-r ../resources/requirements.txt (line 3)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from matplotlib->-r ../resources/requirements.txt (line 3)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from matplotlib->-r ../resources/requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from matplotlib->-r ../resources/requirements.txt (line 3)) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from plotly->-r ../resources/requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from nltk->-r ../resources/requirements.txt (line 8)) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from nltk->-r ../resources/requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from nltk->-r ../resources/requirements.txt (line 8)) (2025.7.34)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (80.3.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from spacy->-r ../resources/requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from tqdm->-r ../resources/requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r ../resources/requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r ../resources/requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r ../resources/requirements.txt (line 9)) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r ../resources/requirements.txt (line 9)) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r ../resources/requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../resources/requirements.txt (line 9)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../resources/requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../resources/requirements.txt (line 9)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../resources/requirements.txt (line 9)) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r ../resources/requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r ../resources/requirements.txt (line 9)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->-r ../resources/requirements.txt (line 9)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->-r ../resources/requirements.txt (line 9)) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r ../resources/requirements.txt (line 9)) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r ../resources/requirements.txt (line 9)) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r ../resources/requirements.txt (line 9)) (1.17.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from scikit-learn->-r ../resources/requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from torch->-r ../resources/requirements.txt (line 14)) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from torch->-r ../resources/requirements.txt (line 14)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from torch->-r ../resources/requirements.txt (line 14)) (3.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from torch->-r ../resources/requirements.txt (line 14)) (2025.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from transformers->-r ../resources/requirements.txt (line 15)) (0.34.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from transformers->-r ../resources/requirements.txt (line 15)) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from transformers->-r ../resources/requirements.txt (line 15)) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from transformers->-r ../resources/requirements.txt (line 15)) (0.6.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r ../resources/requirements.txt (line 9)) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r ../resources/requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r ../resources/requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r ../resources/requirements.txt (line 9)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r ../resources/requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch->-r ../resources/requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dvirl\\pycharmprojects\\semanticanalysis\\.venv\\lib\\site-packages (from jinja2->spacy->-r ../resources/requirements.txt (line 9)) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# Any results you write to the current directory are saved as output."
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "ExecuteTime": {
     "end_time": "2025-08-16T20:03:16.148244Z",
     "start_time": "2025-08-16T20:03:07.286423Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "**Below is a helper Function which generates random colors which can be used to give different colors to your plots.Feel free to use it**",
   "metadata": {}
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true,
    "_kg_hide-input": true,
    "ExecuteTime": {
     "end_time": "2025-08-16T20:03:16.247885Z",
     "start_time": "2025-08-16T20:03:16.159736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../resources/data/Twitter_Emotion_Dataset.csv')\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        label                                              tweet\n",
       "0       anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...\n",
       "1       anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...\n",
       "2       happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...\n",
       "3       anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...\n",
       "4       happy  Sharing pengalaman aja, kemarin jam 18.00 bata...\n",
       "...       ...                                                ...\n",
       "4396     love  Tahukah kamu, bahwa saat itu papa memejamkan m...\n",
       "4397     fear  Sulitnya menetapkan Calon Wapresnya Jokowi di ...\n",
       "4398    anger  5. masa depannya nggak jelas. lha iya, gimana ...\n",
       "4399    happy  [USERNAME] dulu beneran ada mahasiswa Teknik U...\n",
       "4400  sadness  Ya Allah, hanya Engkau yang mengetahui rasa sa...\n",
       "\n",
       "[4401 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>love</td>\n",
       "      <td>Tahukah kamu, bahwa saat itu papa memejamkan m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>fear</td>\n",
       "      <td>Sulitnya menetapkan Calon Wapresnya Jokowi di ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>anger</td>\n",
       "      <td>5. masa depannya nggak jelas. lha iya, gimana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>happy</td>\n",
       "      <td>[USERNAME] dulu beneran ada mahasiswa Teknik U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Ya Allah, hanya Engkau yang mengetahui rasa sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "t## Initial modal"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:03:16.435025Z",
     "start_time": "2025-08-16T20:03:16.263117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data['tweet'] = data['tweet'].apply(lambda x: clean_text(x))\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        label                                              tweet\n",
       "0       anger  soal jln jatibarupolisi tdk bs gertak gubernur...\n",
       "1       anger  sesama cewe lho kayaknya harusnya bisa lebih r...\n",
       "2       happy  kepingin gudeg mbarek bu hj amad foto dari goo...\n",
       "3       anger  jln jatibarubagian dari wilayah tn abangpengat...\n",
       "4       happy  sharing pengalaman aja kemarin jam  batalin ti...\n",
       "...       ...                                                ...\n",
       "4396     love  tahukah kamu bahwa saat itu papa memejamkan ma...\n",
       "4397     fear  sulitnya menetapkan calon wapresnya jokowi di ...\n",
       "4398    anger   masa depannya nggak jelas lha iya gimana mau ...\n",
       "4399    happy   dulu beneran ada mahasiswa teknik ui nembak p...\n",
       "4400  sadness  ya allah hanya engkau yang mengetahui rasa sak...\n",
       "\n",
       "[4401 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>soal jln jatibarupolisi tdk bs gertak gubernur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>sesama cewe lho kayaknya harusnya bisa lebih r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>kepingin gudeg mbarek bu hj amad foto dari goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>jln jatibarubagian dari wilayah tn abangpengat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>sharing pengalaman aja kemarin jam  batalin ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>love</td>\n",
       "      <td>tahukah kamu bahwa saat itu papa memejamkan ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>fear</td>\n",
       "      <td>sulitnya menetapkan calon wapresnya jokowi di ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>anger</td>\n",
       "      <td>masa depannya nggak jelas lha iya gimana mau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>happy</td>\n",
       "      <td>dulu beneran ada mahasiswa teknik ui nembak p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ya allah hanya engkau yang mengetahui rasa sak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:03:18.526346Z",
     "start_time": "2025-08-16T20:03:16.464179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "indo_stop_words = stopwords.words(fileids='indonesian')\n",
    "indo_stop_words.append(\"username\")\n",
    "indo_stop_words.append(\"url\")\n",
    "indo_stop_words.append(\"ya\")\n",
    "indo_stop_words.append(\"yg\")\n",
    "\n",
    "\n",
    "def remove_stopword(x):\n",
    "    return \" \".join([y for y in x.split() if y not in indo_stop_words])\n",
    "\n",
    "\n",
    "data['tweet'] = data['tweet'].apply(lambda x: remove_stopword(x))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:03:57.077483Z",
     "start_time": "2025-08-16T20:03:57.064926Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        label                                              tweet\n",
       "0       anger  jln jatibarupolisi tdk bs gertak gubernur eman...\n",
       "1       anger  cewe lho kayaknya rasain sibuk jaga rasain sak...\n",
       "2       happy  kepingin gudeg mbarek bu hj amad foto google s...\n",
       "3       anger  jln jatibarubagian wilayah tn abangpengaturan ...\n",
       "4       happy  sharing pengalaman aja kemarin jam batalin tik...\n",
       "...       ...                                                ...\n",
       "4396     love  tahukah papa memejamkan matanya menahan gejola...\n",
       "4397     fear  sulitnya menetapkan calon wapresnya jokowi pil...\n",
       "4398    anger  depannya nggak lha iya gimana coba lulusan sen...\n",
       "4399    happy  beneran mahasiswa teknik ui nembak pacarnya pa...\n",
       "4400  sadness         allah engkau sakit hati sembuhkanlah allah\n",
       "\n",
       "[4401 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>jln jatibarupolisi tdk bs gertak gubernur eman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>cewe lho kayaknya rasain sibuk jaga rasain sak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>kepingin gudeg mbarek bu hj amad foto google s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>jln jatibarubagian wilayah tn abangpengaturan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>sharing pengalaman aja kemarin jam batalin tik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>love</td>\n",
       "      <td>tahukah papa memejamkan matanya menahan gejola...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>fear</td>\n",
       "      <td>sulitnya menetapkan calon wapresnya jokowi pil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>anger</td>\n",
       "      <td>depannya nggak lha iya gimana coba lulusan sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>happy</td>\n",
       "      <td>beneran mahasiswa teknik ui nembak pacarnya pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>sadness</td>\n",
       "      <td>allah engkau sakit hati sembuhkanlah allah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:08:39.190175Z",
     "start_time": "2025-08-16T20:08:30.183403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Encode labels\n",
    "encode_labels = LabelEncoder().fit_transform(data[\"label\"])\n",
    "# Train-test split\n",
    "\n",
    "df = data\n",
    "\n",
    "X = df[\"tweet\"]\n",
    "y = encode_labels\n",
    "\n",
    "# 1. TF-IDF vectorizer (turns tweets into vectors)\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),       # unigrams + bigrams\n",
    "    stop_words=None          # you can plug in Indonesian stopword list here\n",
    ")\n",
    "\n",
    "# 2. LSA via Truncated SVD\n",
    "lsa = TruncatedSVD(n_components=100, random_state=42)\n",
    "\n",
    "# 3. Classifier\n",
    "clf = XGBClassifier(random_state=42)\n",
    "\n",
    "# 4. Pipeline\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"lsa\", lsa),\n",
    "    (\"clf\", clf)\n",
    "])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59       119\n",
      "           1       0.69      0.56      0.62        62\n",
      "           2       0.65      0.51      0.57       111\n",
      "           3       0.67      0.87      0.76        55\n",
      "           4       0.41      0.41      0.41        94\n",
      "\n",
      "    accuracy                           0.58       441\n",
      "   macro avg       0.59      0.60      0.59       441\n",
      "weighted avg       0.58      0.58      0.57       441\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trying basic NN"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "X_text = df[\"tweet\"].values\n",
    "y = encode_labels\n",
    "\n",
    "# --------------------------\n",
    "# TF-IDF + LSA\n",
    "# --------------------------\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "lsa = TruncatedSVD(n_components=100, random_state=42)\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(X_text)\n",
    "X_lsa = lsa.fit_transform(X_tfidf).astype(np.float32)   # (n_samples, 100)\n",
    "X_tfidf, X_lsa"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:13:07.073238Z",
     "start_time": "2025-08-16T20:13:06.488697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       " \twith 54663 stored elements and shape (4401, 5000)>,\n",
       " array([[ 0.02715662,  0.00668656, -0.0214614 , ...,  0.0580422 ,\n",
       "          0.014818  , -0.06430656],\n",
       "        [ 0.08823814, -0.01614558, -0.00832853, ...,  0.03948399,\n",
       "         -0.04237257, -0.0142221 ],\n",
       "        [ 0.05108306,  0.01388742, -0.02614813, ...,  0.05248163,\n",
       "          0.02892105,  0.08285797],\n",
       "        ...,\n",
       "        [ 0.11985517, -0.02876377, -0.05878504, ..., -0.00741188,\n",
       "         -0.04522653,  0.0497761 ],\n",
       "        [ 0.08878586,  0.08303224, -0.00087154, ...,  0.00849323,\n",
       "          0.03101327,  0.01118153],\n",
       "        [ 0.08734218,  0.07794479, -0.03971541, ..., -0.04679065,\n",
       "         -0.00307968, -0.06319682]], shape=(4401, 100), dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T20:13:08.243779Z",
     "start_time": "2025-08-16T20:13:08.225748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, text_features, labels):\n",
    "        self.text_features = torch.tensor(text_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_features[idx], self.labels[idx]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lsa, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TweetDataset(X_train, y_train)\n",
    "test_dataset = TweetDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "train_loader, test_loader"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x210cddeed10>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x210cde15c10>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T21:05:13.258825Z",
     "start_time": "2025-08-16T21:05:13.159343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassifierBiLSTM(nn.Module):\n",
    "    def __init__(self, text_input_dim, hidden_dim, lstm_hidden_dim, output_dim, seq_len=10):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.feature_dim = text_input_dim // seq_len  # split LSA into timesteps\n",
    "\n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text_x):\n",
    "        # text_x: (batch, text_input_dim)\n",
    "        batch_size = text_x.size(0)\n",
    "\n",
    "        # reshape into (batch, seq_len, feature_dim)\n",
    "        x = text_x.view(batch_size, self.seq_len, self.feature_dim)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)   # (batch, seq_len, 2*lstm_hidden_dim)\n",
    "\n",
    "        # take the last timestep output\n",
    "        last_out = lstm_out[:, -1, :]   # (batch, 2*lstm_hidden_dim)\n",
    "\n",
    "        # fully connected\n",
    "        h = F.relu(self.fc1(last_out))\n",
    "        out = self.fc2(h)\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T21:39:31.634214Z",
     "start_time": "2025-08-16T21:25:23.953996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --------------------------\n",
    "# Training\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifierBiLSTM(\n",
    "    text_input_dim=X_lsa.shape[1],  # must match LSA dims (e.g., 100)\n",
    "    hidden_dim=64,\n",
    "    lstm_hidden_dim=32,\n",
    "    output_dim=len(set(y)),\n",
    "    seq_len=10                      # e.g., 10 timesteps × 10 features = 100\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for text_x, labels in train_loader:\n",
    "        text_x, labels = text_x.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text_x)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5681\n",
      "Epoch 2, Loss: 1.5137\n",
      "Epoch 3, Loss: 1.5206\n",
      "Epoch 4, Loss: 1.6745\n",
      "Epoch 5, Loss: 1.4555\n",
      "Epoch 6, Loss: 1.8816\n",
      "Epoch 7, Loss: 1.4900\n",
      "Epoch 8, Loss: 1.4382\n",
      "Epoch 9, Loss: 1.4840\n",
      "Epoch 10, Loss: 1.4793\n",
      "Epoch 11, Loss: 1.9024\n",
      "Epoch 12, Loss: 1.4395\n",
      "Epoch 13, Loss: 1.4382\n",
      "Epoch 14, Loss: 1.4873\n",
      "Epoch 15, Loss: 1.3965\n",
      "Epoch 16, Loss: 1.6558\n",
      "Epoch 17, Loss: 1.6983\n",
      "Epoch 18, Loss: 1.4408\n",
      "Epoch 19, Loss: 1.3997\n",
      "Epoch 20, Loss: 1.6875\n",
      "Epoch 21, Loss: 1.8952\n",
      "Epoch 22, Loss: 1.4820\n",
      "Epoch 23, Loss: 1.9063\n",
      "Epoch 24, Loss: 1.6837\n",
      "Epoch 25, Loss: 1.9204\n",
      "Epoch 26, Loss: 1.4897\n",
      "Epoch 27, Loss: 1.7014\n",
      "Epoch 28, Loss: 1.4822\n",
      "Epoch 29, Loss: 1.4396\n",
      "Epoch 30, Loss: 1.3939\n",
      "Epoch 31, Loss: 1.4844\n",
      "Epoch 32, Loss: 1.6548\n",
      "Epoch 33, Loss: 1.9036\n",
      "Epoch 34, Loss: 1.9167\n",
      "Epoch 35, Loss: 1.4404\n",
      "Epoch 36, Loss: 1.6860\n",
      "Epoch 37, Loss: 1.6984\n",
      "Epoch 38, Loss: 1.6959\n",
      "Epoch 39, Loss: 1.3964\n",
      "Epoch 40, Loss: 1.4338\n",
      "Epoch 41, Loss: 1.4831\n",
      "Epoch 42, Loss: 1.4355\n",
      "Epoch 43, Loss: 1.4813\n",
      "Epoch 44, Loss: 1.6832\n",
      "Epoch 45, Loss: 1.6565\n",
      "Epoch 46, Loss: 1.7014\n",
      "Epoch 47, Loss: 1.6834\n",
      "Epoch 48, Loss: 1.4335\n",
      "Epoch 49, Loss: 1.9069\n",
      "Epoch 50, Loss: 1.6918\n",
      "Epoch 51, Loss: 1.4388\n",
      "Epoch 52, Loss: 1.6457\n",
      "Epoch 53, Loss: 1.6987\n",
      "Epoch 54, Loss: 1.9167\n",
      "Epoch 55, Loss: 1.3924\n",
      "Epoch 56, Loss: 1.4794\n",
      "Epoch 57, Loss: 1.6872\n",
      "Epoch 58, Loss: 1.4415\n",
      "Epoch 59, Loss: 1.4355\n",
      "Epoch 60, Loss: 1.6979\n",
      "Epoch 61, Loss: 1.6885\n",
      "Epoch 62, Loss: 1.9048\n",
      "Epoch 63, Loss: 1.4790\n",
      "Epoch 64, Loss: 1.6472\n",
      "Epoch 65, Loss: 1.6856\n",
      "Epoch 66, Loss: 1.6890\n",
      "Epoch 67, Loss: 1.6434\n",
      "Epoch 68, Loss: 1.6994\n",
      "Epoch 69, Loss: 1.4398\n",
      "Epoch 70, Loss: 1.8935\n",
      "Epoch 71, Loss: 1.4345\n",
      "Epoch 72, Loss: 1.6857\n",
      "Epoch 73, Loss: 1.6977\n",
      "Epoch 74, Loss: 1.4384\n",
      "Epoch 75, Loss: 1.4792\n",
      "Epoch 76, Loss: 1.4396\n",
      "Epoch 77, Loss: 1.7001\n",
      "Epoch 78, Loss: 1.6907\n",
      "Epoch 79, Loss: 1.9081\n",
      "Epoch 80, Loss: 1.7010\n",
      "Epoch 81, Loss: 1.4380\n",
      "Epoch 82, Loss: 1.6575\n",
      "Epoch 83, Loss: 1.6465\n",
      "Epoch 84, Loss: 1.6454\n",
      "Epoch 85, Loss: 1.6549\n",
      "Epoch 86, Loss: 1.6872\n",
      "Epoch 87, Loss: 1.9031\n",
      "Epoch 88, Loss: 1.6864\n",
      "Epoch 89, Loss: 1.6461\n",
      "Epoch 90, Loss: 1.6908\n",
      "Epoch 91, Loss: 1.6945\n",
      "Epoch 92, Loss: 1.6915\n",
      "Epoch 93, Loss: 1.9176\n",
      "Epoch 94, Loss: 1.6560\n",
      "Epoch 95, Loss: 1.6854\n",
      "Epoch 96, Loss: 1.7023\n",
      "Epoch 97, Loss: 1.4403\n",
      "Epoch 98, Loss: 1.4390\n",
      "Epoch 99, Loss: 1.6548\n",
      "Epoch 100, Loss: 1.4774\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T21:24:33.465174Z",
     "start_time": "2025-08-16T21:24:32.830580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Evaluation\n",
    "# --------------------------\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for text_x, labels in test_loader:\n",
    "        text_x, labels = text_x.to(device), labels.to(device)\n",
    "        outputs = model(text_x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Accuracy:\", correct / total)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5164585698070374\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
